{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOpxyT8WNjK295ZSs0l1rhx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ECevCiRfuI2w","executionInfo":{"status":"ok","timestamp":1748852751537,"user_tz":-540,"elapsed":1518,"user":{"displayName":"indi sol","userId":"02918963309027256120"}},"outputId":"5a5a34e0-9fbf-4d54-8699-dafaad7b2631"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["!pip install ultralytics\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26k4xPfyuTDn","executionInfo":{"status":"ok","timestamp":1748852758196,"user_tz":-540,"elapsed":6656,"user":{"displayName":"indi sol","userId":"02918963309027256120"}},"outputId":"1fb3a7d6-fbbf-4c79-8b9b-d3c7dc4b8777"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.147)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"]}]},{"cell_type":"code","source":["import os\n","import time\n","from ultralytics import YOLO\n","import cv2\n","\n","# ‚úÖ Î™®Îç∏ Í≤ΩÎ°ú (.pt ÌååÏùºÍπåÏßÄ)\n","model_path = \"/content/drive/MyDrive/capstone/yolov5_2nd_version_train/yolov5n_ep100_bs16_img960/weights/best.pt\"\n","image_dir = \"/content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images\"\n","\n","# ‚úÖ Î™®Îç∏ Î°úÎî©\n","model = YOLO(model_path)\n","\n","# ‚úÖ Ïù¥ÎØ∏ÏßÄ Î™©Î°ù\n","image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.png'))]\n","image_paths = [os.path.join(image_dir, f) for f in image_files]\n","\n","# ‚úÖ FPS Ï∏°Ï†ï\n","start = time.time()\n","for img_path in image_paths:\n","    model(img_path)\n","end = time.time()\n","\n","# ‚úÖ Í≥ÑÏÇ∞\n","total = len(image_paths)\n","elapsed = end - start\n","fps = total / elapsed\n","\n","# ‚úÖ ÌèâÍ∞Ä\n","if fps >= 30:\n","    grade = \"üèÖ Îß§Ïö∞ Ïö∞Ïàò (CCTV ÎåÄÏùë Í∞ÄÎä•)\"\n","elif fps >= 15:\n","    grade = \"üëç ÏñëÌò∏ (Ïã§ÏãúÍ∞Ñ Í∞ÄÎä•)\"\n","elif fps >= 5:\n","    grade = \"‚ö†Ô∏è Î≥¥ÌÜµ (ÏÇ¨ÌõÑ Î∂ÑÏÑùÏö©)\"\n","else:\n","    grade = \"‚ùå ÎäêÎ¶º (ÏóÖÍ∑∏Î†àÏù¥Îìú ÌïÑÏöî)\"\n","\n","# ‚úÖ Ï∂úÎ†•\n","print(f\"\\nüìä FPS ÏÑ±Îä• ÌèâÍ∞Ä Í≤∞Í≥º\\n{'-'*40}\")\n","print(f\"Ï¥ù Ïù¥ÎØ∏ÏßÄ Ïàò     : {total}\")\n","print(f\"Ï¥ù ÏÜåÏöî ÏãúÍ∞Ñ     : {elapsed:.2f}Ï¥à\")\n","print(f\"ÌèâÍ∑† FPS         : {fps:.2f}\")\n","print(f\"ÏÑ±Îä• ÌèâÍ∞Ä        : {grade}\")\n","print(f\"{'-'*40}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zm556iLJuPXr","executionInfo":{"status":"ok","timestamp":1748853751069,"user_tz":-540,"elapsed":3212,"user":{"displayName":"indi sol","userId":"02918963309027256120"}},"outputId":"f30e3a98-2643-44aa-dc49-9d45c43aa308"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_003900_jpg.rf.06fe6115c628ec86952551b3a9fa853c.jpg: 640x640 1 gown_on, 4 hairnet_ons, 6 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_003540_jpg.rf.9e6ed26ccc834055586104e48af7b389.jpg: 640x640 3 gown_ons, 3 hairnet_ons, 4 persons, 7.9ms\n","Speed: 2.2ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_002100_jpg.rf.e99991537773641016ab9f0ef5f8cb59.jpg: 640x640 6 gown_ons, 9 hairnet_ons, 12 persons, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_001065_jpg.rf.30c387f314d6c1653f761a1f5b66de9c.jpg: 640x640 4 gown_ons, 5 hairnet_ons, 6 persons, 10.3ms\n","Speed: 1.7ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_000390_jpg.rf.6f7f580c9d71f7b3f8746a024de773bc.jpg: 640x640 2 gown_ons, 2 hairnet_ons, 2 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_000120_jpg.rf.551e16c2ab8cb2ad28b6a400dba9e85c.jpg: 640x640 2 gown_ons, 4 hairnet_ons, 4 persons, 10.1ms\n","Speed: 2.3ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_000420_jpg.rf.e94f581e0e12617920ba8238faed8802.jpg: 640x640 2 gown_ons, 4 hairnet_ons, 3 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_002610_jpg.rf.79b224edefee4710b638667a6a358e42.jpg: 640x640 3 gown_ons, 5 hairnet_ons, 6 persons, 10.7ms\n","Speed: 1.8ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_002670_jpg.rf.b6e9b3d2a78c2e0881e340bd76c47de0.jpg: 640x640 4 gown_ons, 6 hairnet_ons, 6 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_002280_jpg.rf.57c74d63d4e7225bc8d63e12d4c06662.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.7ms\n","Speed: 1.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_000765_jpg.rf.3f943cbeb7133d83f0538f05f1aa8834.jpg: 640x640 4 gown_ons, 4 hairnet_ons, 5 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_000570_jpg.rf.c4f4b3967d2ea5c2dd408247712cf149.jpg: 640x640 1 gown_on, 3 hairnet_ons, 4 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_007770_jpg.rf.c27bd9c630b379d81447b24411d6e4ff.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_007470_jpg.rf.6b6ff7b5d9e6a694a37bdd5c2c503ec6.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005220_jpg.rf.32a8dbdeff5350e2a943aafd98601aad.jpg: 640x640 2 gown_ons, 4 hairnet_ons, 5 persons, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_007620_jpg.rf.1e6aff5af99516a71bf44ea2f3ef20e8.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.5ms\n","Speed: 1.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005610_jpg.rf.1986d06abe6bdccf134eb17e29b5ac84.jpg: 640x640 3 gown_ons, 5 hairnet_ons, 6 persons, 7.6ms\n","Speed: 1.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_006720_jpg.rf.0a8e78637eccb83be07647842bc73a99.jpg: 640x640 1 gown_on, 1 hairnet_on, 2 persons, 7.7ms\n","Speed: 1.7ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_013530_jpg.rf.3c35877dd0d9d62712d7f354f15d73eb.jpg: 640x640 2 gown_ons, 2 hairnet_ons, 2 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_012615_jpg.rf.71fd8eaab950ef1fd7ed6d710491b30f.jpg: 640x640 2 gown_ons, 5 hairnet_ons, 4 persons, 7.5ms\n","Speed: 2.2ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011700_jpg.rf.383c90e9479d1d28abb776f9dc5fcc8d.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_013260_jpg.rf.135bdd89949d27046a89dcaccc17e0a4.jpg: 640x640 2 gown_ons, 4 hairnet_ons, 2 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_012600_jpg.rf.a5665dd993afba9577b3f663c01bc9d1.jpg: 640x640 2 gown_ons, 4 hairnet_ons, 6 persons, 7.2ms\n","Speed: 1.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011265_jpg.rf.467d3fe02fdbb75c99676dad4d219494.jpg: 640x640 3 gown_ons, 6 hairnet_ons, 5 persons, 7.4ms\n","Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011865_jpg.rf.08cd9d9c5fdef7ecf1a9a130ec0039ee.jpg: 640x640 2 gown_ons, 5 hairnet_ons, 5 persons, 7.2ms\n","Speed: 1.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_010890_jpg.rf.b877f94e87c11d8c2487ce4e449c5645.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.2ms\n","Speed: 1.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011250_jpg.rf.57fd7e33f446a04c1115cfef203924fb.jpg: 640x640 2 gown_ons, 5 hairnet_ons, 4 persons, 8.7ms\n","Speed: 1.6ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_009240_jpg.rf.202b3b64d92beec194cbbaa687f78228.jpg: 640x640 1 person, 16.6ms\n","Speed: 2.7ms preprocess, 16.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011490_jpg.rf.e951aedcdf46ec3b2b30d9c0eb9e7cf0.jpg: 640x640 3 gown_ons, 6 hairnet_ons, 5 persons, 17.1ms\n","Speed: 2.4ms preprocess, 17.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_009270_jpg.rf.e060b717bf94495072c0eabb6707a5cd.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 15.5ms\n","Speed: 2.2ms preprocess, 15.5ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011250_jpg.rf.ebdab796f85d8cd2c591391e40f4345b.jpg: 640x640 1 person, 9.7ms\n","Speed: 3.0ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008160_jpg.rf.91ce6def0c3dc1f34bd5107722e98b27.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 25.3ms\n","Speed: 2.3ms preprocess, 25.3ms inference, 9.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011280_jpg.rf.268debcf6aeabe2919a698ec61688d75.jpg: 640x640 1 person, 17.1ms\n","Speed: 2.4ms preprocess, 17.1ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_009810_jpg.rf.bed2660a4bd22c16a950a02ee7aecfb6.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 20.1ms\n","Speed: 2.2ms preprocess, 20.1ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_009090_jpg.rf.9cc596dfb0980b65b0f6f0721c9457a6.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 18.5ms\n","Speed: 2.6ms preprocess, 18.5ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_009780_jpg.rf.919d224ac539ca78e5f003e064e8c6c3.jpg: 640x640 2 gown_ons, 6 hairnet_ons, 5 persons, 15.6ms\n","Speed: 2.2ms preprocess, 15.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008220_jpg.rf.bab0e5cfde4edc04cd8e1ddc38a904df.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 9.5ms\n","Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011670_jpg.rf.81bb28f35f21efb369eca6d52a99aeed.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011505_jpg.rf.1dba949586f543c19a976519a1ac1967.jpg: 640x640 3 gown_ons, 5 hairnet_ons, 5 persons, 7.3ms\n","Speed: 1.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_010560_jpg.rf.135cfce044e3750ff2bdf1424d54fe2e.jpg: 640x640 3 gown_ons, 5 hairnet_ons, 5 persons, 7.3ms\n","Speed: 1.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_010710_jpg.rf.20dd5d7c11ae08364f90f9b2c4ccfd53.jpg: 640x640 1 gown_on, 1 person, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008790_jpg.rf.e48a0fdcc3171422764aac0ba4bfaed5.jpg: 640x640 2 persons, 7.2ms\n","Speed: 1.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_007920_jpg.rf.e597005d6d0cf8ad5dad218e4b5f6fa6.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.2ms\n","Speed: 1.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_007830_jpg.rf.a703229e758be0c072f649e04d7b9d75.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008040_jpg.rf.726485215c8b0524b18331d52bfa9f11.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008460_jpg.rf.0aa9ab109691e4234ddb275209c74283.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008130_jpg.rf.99490738b7f082ea172c40b20fc6e03a.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005250_jpg.rf.06086c96b5cfdb901779730f1fdd477c.jpg: 640x640 2 gown_ons, 5 hairnet_ons, 5 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_004080_jpg.rf.8bc55d6e218e6cfc665cf4b2ab1d8d0a.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_004200_jpg.rf.af89ea3763be8c7622685338a580d5f9.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.4ms\n","Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005730_jpg.rf.4cff361c34e5d2416f2bf6e7a6e4b36d.jpg: 640x640 1 hairnet_on, 1 person, 9.1ms\n","Speed: 1.8ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005940_jpg.rf.674e3706525111ffe6337003635748c0.jpg: 640x640 2 gown_ons, 1 hairnet_on, 2 persons, 7.4ms\n","Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_004020_jpg.rf.cb86c1ef6997892ff51d6f53148da87e.jpg: 640x640 1 gown_on, 1 person, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005700_jpg.rf.9086396df37d8f8b3894ebde2faf2691.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.2ms\n","Speed: 1.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_004455_jpg.rf.0a24fbe290b51d36228e56808bdbd151.jpg: 640x640 1 gown_on, 4 hairnet_ons, 4 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_006345_jpg.rf.800cc35401cf9f6ccf57111f7fe117b1.jpg: 640x640 1 gown_on, 4 hairnet_ons, 3 persons, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_004110_jpg.rf.aa0273b0afee1249cdf0b0c8ec19d459.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 8.6ms\n","Speed: 2.2ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_003870_jpg.rf.c43ad041f9919eb2b0f5486c2827001e.jpg: 640x640 2 gown_ons, 1 hairnet_on, 2 persons, 7.8ms\n","Speed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_002130_jpg.rf.231d276a39bb109cefd61e91bd952d43.jpg: 640x640 4 gown_ons, 9 hairnet_ons, 9 persons, 7.9ms\n","Speed: 1.6ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_001770_jpg.rf.63b5efd09e2dd5a59b9d89d0e0b09a34.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.4ms\n","Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_006660_jpg.rf.f3656c1181f25c2ba0903627be2db732.jpg: 640x640 2 gown_ons, 5 hairnet_ons, 4 persons, 7.6ms\n","Speed: 1.7ms preprocess, 7.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_028410_jpg.rf.899f1e42354ece84bcbd72c0e25d0bd1.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.5ms\n","Speed: 1.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_028200_jpg.rf.3938a39f46bef9a9449363fc70fc1773.jpg: 640x640 2 gown_ons, 1 hairnet_on, 2 persons, 7.8ms\n","Speed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_027570_jpg.rf.92cbb5ee86f1e0b2fb5af813515ae660.jpg: 640x640 1 gown_on, 1 person, 9.4ms\n","Speed: 1.6ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_027645_jpg.rf.06928ef4e6b3a849eecbe25cc911704b.jpg: 640x640 1 gown_on, 1 person, 7.2ms\n","Speed: 1.8ms preprocess, 7.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_026685_jpg.rf.ec067fd51b4b6152d833667092743ed3.jpg: 640x640 1 gown_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_026655_jpg.rf.447f90f0a436bd1b5d89a28260721259.jpg: 640x640 1 gown_on, 1 person, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_024525_jpg.rf.58948ce8dafc91bbf3f8a121476ec15c.jpg: 640x640 1 gown_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_026640_jpg.rf.43020d85eacc52184ca830ad91eff070.jpg: 640x640 1 gown_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022350_jpg.rf.7be7215a10c97f566e2290b82fa29dae.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_021600_jpg.rf.5bcda8a28f60e935e235513c588f1999.jpg: 640x640 2 gown_ons, 2 hairnet_ons, 3 persons, 8.4ms\n","Speed: 1.7ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_021750_jpg.rf.8724b07e796175ffd88179f05a1a9e3b.jpg: 640x640 2 gown_ons, 2 hairnet_ons, 2 persons, 7.2ms\n","Speed: 1.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022380_jpg.rf.7ad0069737c77e0bee1687db2c85341f.jpg: 640x640 3 gown_ons, 4 hairnet_ons, 4 persons, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_021555_jpg.rf.bc321a759dc102c1e1a62d453f43f063.jpg: 640x640 2 gown_ons, 2 hairnet_ons, 2 persons, 9.1ms\n","Speed: 1.6ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022620_jpg.rf.2670dcfc2f768d5cc819ef29d0cb3b5a.jpg: 640x640 1 gown_on, 1 person, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_021480_jpg.rf.345b0c6c0d4019a885111cdbb84eb6db.jpg: 640x640 3 gown_ons, 2 hairnet_ons, 2 persons, 14.3ms\n","Speed: 2.5ms preprocess, 14.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022515_jpg.rf.95f94f9d8ad972e8c62704b1a737069c.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 9.0ms\n","Speed: 5.5ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022500_jpg.rf.556759515a8b8247e9fceb4faa335cde.jpg: 640x640 2 gown_ons, 4 hairnet_ons, 4 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_027525_jpg.rf.effd79cad391f0c65d383201ffb48e51.jpg: 640x640 1 gown_on, 1 person, 7.8ms\n","Speed: 2.2ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_024255_jpg.rf.d1f1069a35f5dedd5a87c8470a01bd2a.jpg: 640x640 1 gown_on, 1 person, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_026715_jpg.rf.5e7a5effd2af1f767a438d0e4dcb8b13.jpg: 640x640 1 gown_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022665_jpg.rf.58a5d1797c71d7fa4f731673441b24aa.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.5ms\n","Speed: 1.7ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_023370_jpg.rf.506e4ac41b5e5ca18666cab55f79dd45.jpg: 640x640 1 gown_on, 2 hairnet_ons, 2 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_025215_jpg.rf.752bb28aa5fac15be09b9742f3a22968.jpg: 640x640 1 gown_on, 1 person, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_023940_jpg.rf.b5119e06fed50378aa027f74edc84d6d.jpg: 640x640 1 gown_on, 1 person, 7.3ms\n","Speed: 2.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_024510_jpg.rf.4051092856f9fbfd68c309b738fb194a.jpg: 640x640 1 gown_on, 1 person, 7.4ms\n","Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_023100_jpg.rf.fc7faf98b1bbe36073850a69b48b3f0f.jpg: 640x640 2 gown_ons, 3 hairnet_ons, 3 persons, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020730_jpg.rf.8cbfe8aa0f5dd7860662244584d15c79.jpg: 640x640 1 gown_on, 2 hairnet_ons, 2 persons, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_021510_jpg.rf.874e88b869e35bd60c8e562c50d226fc.jpg: 640x640 4 gown_ons, 4 hairnet_ons, 5 persons, 8.0ms\n","Speed: 1.7ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020475_jpg.rf.08b49c17e991643da30ac760b071c9a0.jpg: 640x640 1 person, 7.3ms\n","Speed: 1.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020250_jpg.rf.3235409a730b0190afda22efb4ca6ea9.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.4ms\n","Speed: 1.6ms preprocess, 7.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020775_jpg.rf.604861e9010100849057c323e3e9fca2.jpg: 640x640 1 gown_on, 1 hairnet_on, 2 persons, 7.3ms\n","Speed: 1.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020640_jpg.rf.99f6dec3ef6c5671422d1f42ec975c8b.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.2ms\n","Speed: 1.6ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020235_jpg.rf.3b7a6d851c67f723065ad8843215a0f5.jpg: 640x640 5 gown_ons, 5 hairnet_ons, 5 persons, 8.2ms\n","Speed: 2.5ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_016260_jpg.rf.4c819edc1cf671ffc36defdc67a3269c.jpg: 640x640 5 gown_ons, 7 hairnet_ons, 6 persons, 8.5ms\n","Speed: 1.6ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_018630_jpg.rf.f9d317ef5a793e2071fbcc096ef1bff7.jpg: 640x640 1 gown_on, 1 person, 7.5ms\n","Speed: 1.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_018690_jpg.rf.722708915f463775e556315904125843.jpg: 640x640 1 gown_on, 1 person, 9.0ms\n","Speed: 1.9ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_019290_jpg.rf.0e388c392a34d1eec4de4873e2f12de5.jpg: 640x640 2 gown_ons, 1 hairnet_on, 3 persons, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_018330_jpg.rf.1d3c679c7a49e457df9019e3297e6750.jpg: 640x640 1 gown_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_019500_jpg.rf.7d538b346c9f1d485f260596bd9e40eb.jpg: 640x640 1 gown_on, 1 person, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_015270_jpg.rf.8f666285a948a59ac84fd1f68c51dead.jpg: 640x640 3 gown_ons, 3 hairnet_ons, 3 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_016140_jpg.rf.f13e12c5e40477c846ea3d18dc527b40.jpg: 640x640 3 gown_ons, 3 hairnet_ons, 4 persons, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_019485_jpg.rf.c43bbf492e5f84d1c07ca4b50ed6b201.jpg: 640x640 2 gown_ons, 2 hairnet_ons, 3 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_016620_jpg.rf.4b7a44fab5c9ad4ec224351a81b2e6da.jpg: 640x640 5 gown_ons, 7 hairnet_ons, 7 persons, 7.2ms\n","Speed: 1.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_015600_jpg.rf.009131807dcdc9d075bd66cc15ef86ae.jpg: 640x640 1 hairnet_on, 1 person, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_019080_jpg.rf.e67e6270b2209dadecd52fac8142aebc.jpg: 640x640 1 gown_on, 1 person, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_014160_jpg.rf.5937eb7d6c7a87854fad7da42edb20f8.jpg: 640x640 3 gown_ons, 3 hairnet_ons, 4 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_014070_jpg.rf.7c7d1ce5b498440f3e8660ff14c4b39f.jpg: 640x640 4 gown_ons, 4 hairnet_ons, 5 persons, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_015420_jpg.rf.998440654de69efff75525a09d594b57.jpg: 640x640 3 gown_ons, 4 hairnet_ons, 6 persons, 7.3ms\n","Speed: 1.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_015210_jpg.rf.5a6d06712146d4f36517ea9d68378d7f.jpg: 640x640 4 gown_ons, 4 hairnet_ons, 6 persons, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_015210_jpg.rf.dfb5f424704477838baaa7834da6eb57.jpg: 640x640 2 gown_ons, 2 hairnet_ons, 2 persons, 7.3ms\n","Speed: 1.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_014895_jpg.rf.00e73bbc77ab63a79e58eedb71bb9ad2.jpg: 640x640 3 gown_ons, 4 hairnet_ons, 5 persons, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_013560_jpg.rf.4ee2ec5a4467101cf887dc395960af2b.jpg: 640x640 3 gown_ons, 2 hairnet_ons, 3 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_019620_jpg.rf.ecba99ee042d97fae2f61f1960bae888.jpg: 640x640 1 gown_on, 1 person, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_375_jpg.rf.eedf693ebd9cc19bf500d449b4e5f797.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 11.5ms\n","Speed: 2.1ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_361_jpg.rf.ebd0297fba31589a389f5916c5422f0a.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 12.5ms\n","Speed: 3.2ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_413_jpg.rf.a07122ecea8ca4cb73d001b87fdcbc78.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.8ms\n","Speed: 1.8ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_229_jpg.rf.a1bae4976bbef5cc7c7e60c6d40d8d54.jpg: 640x640 3 gown_ons, 3 hairnet_ons, 4 persons, 7.2ms\n","Speed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_59_jpg.rf.29113bafe5b768556da0c0a6d690cb13.jpg: 640x640 4 gown_ons, 5 hairnet_ons, 6 persons, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_051060_jpg.rf.b58cdc7956b970368bb9d8545b62878b.jpg: 640x640 3 gown_ons, 3 hairnet_ons, 3 persons, 8.0ms\n","Speed: 2.3ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_031260_jpg.rf.b83b4fb64454ed1b071957c14ec2e759.jpg: 640x640 4 gown_ons, 5 hairnet_ons, 4 persons, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_028425_jpg.rf.b101442f6530449a8ea175b89a3ef8e5.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 8.6ms\n","Speed: 2.5ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_029670_jpg.rf.0dd4f6b007cdfdefbf4bf9681a4754f1.jpg: 640x640 2 gown_ons, 1 hairnet_on, 2 persons, 10.8ms\n","Speed: 1.6ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_065610_jpg.rf.d107446bce4d9f01c2ce332cbd1e46b4.jpg: 640x640 3 gown_ons, 4 hairnet_ons, 4 persons, 13.0ms\n","Speed: 2.4ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_71_jpg.rf.3033eab39133d2fe24bd8f1a763e4a92.jpg: 640x640 5 gown_ons, 5 hairnet_ons, 6 persons, 8.1ms\n","Speed: 1.6ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_030705_jpg.rf.268dd4ac80d2f267f6eb53568984f295.jpg: 640x640 1 gown_on, 1 hairnet_on, 1 person, 7.4ms\n","Speed: 1.7ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_029505_jpg.rf.3466a1d91a082f68d4e2d1fb8e7b3dd1.jpg: 640x640 1 gown_on, 1 person, 7.4ms\n","Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_072495_jpg.rf.f6ec06197ccdd337e8d8044f6249977e.jpg: 640x640 3 gown_ons, 4 hairnet_ons, 5 persons, 7.3ms\n","Speed: 1.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_037950_jpg.rf.3bfa91e24bdf91a3d899144e5a8134c8.jpg: 640x640 5 gown_ons, 3 hairnet_ons, 7 persons, 7.8ms\n","Speed: 2.0ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_051030_jpg.rf.9570942c61d4aaea5aea9decea39eaa2.jpg: 640x640 3 gown_ons, 3 hairnet_ons, 3 persons, 7.1ms\n","Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_028365_jpg.rf.9cd8dc4f80e9ab2af1cb344a0e34aefa.jpg: 640x640 1 gown_on, 1 hairnet_on, 2 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_078915_jpg.rf.1bebcf10df34a4fcc9cb6de93ff6f5a2.jpg: 640x640 3 gown_ons, 4 hairnet_ons, 6 persons, 7.5ms\n","Speed: 2.1ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","üìä FPS ÏÑ±Îä• ÌèâÍ∞Ä Í≤∞Í≥º\n","----------------------------------------\n","Ï¥ù Ïù¥ÎØ∏ÏßÄ Ïàò     : 132\n","Ï¥ù ÏÜåÏöî ÏãúÍ∞Ñ     : 3.16Ï¥à\n","ÌèâÍ∑† FPS         : 41.72\n","ÏÑ±Îä• ÌèâÍ∞Ä        : üèÖ Îß§Ïö∞ Ïö∞Ïàò (CCTV ÎåÄÏùë Í∞ÄÎä•)\n","----------------------------------------\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","\n","# ‚úÖ Í≤ΩÎ°ú ÏÑ§Ï†ï\n","model_path = \"/content/drive/MyDrive/capstone/yolov5_2nd_version_train/yolov5n_ep100_bs16_img960_t75v15t10/weights/best.pt\"\n","image_dir = \"/content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images\"\n","label_dir = \"/content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/labels\"\n","\n","model = YOLO(model_path)\n","\n","# ‚úÖ IoU Ìï®Ïàò\n","def compute_iou(box1, box2):\n","    x1, y1, x2, y2 = box1\n","    x1g, y1g, x2g, y2g = box2\n","    xi1, yi1 = max(x1, x1g), max(y1, y1g)\n","    xi2, yi2 = min(x2, x2g), min(y2, y2g)\n","    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n","    box1_area = (x2 - x1) * (y2 - y1)\n","    box2_area = (x2g - x1g) * (y2g - y1g)\n","    union_area = box1_area + box2_area - inter_area\n","    return inter_area / union_area if union_area != 0 else 0\n","\n","# ‚úÖ ÌèâÍ∞Ä Î£®ÌîÑ\n","TP, FP, FN = 0, 0, 0\n","image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n","\n","for fname in image_files:\n","    img_path = os.path.join(image_dir, fname)\n","    label_path = os.path.join(label_dir, os.path.splitext(fname)[0] + '.txt')\n","    img = cv2.imread(img_path)\n","    h, w = img.shape[:2]\n","\n","    # üîç ÏòàÏ∏° Í≤∞Í≥º\n","    preds = model(img_path)[0].boxes.xyxy.cpu().numpy()\n","\n","    # üîç GT ÎùºÎ≤®\n","    gts = []\n","    if os.path.exists(label_path):\n","        with open(label_path, 'r') as f:\n","            for line in f:\n","                cls, x, y, bw, bh = map(float, line.strip().split())\n","                x1 = (x - bw / 2) * w\n","                y1 = (y - bh / 2) * h\n","                x2 = (x + bw / 2) * w\n","                y2 = (y + bh / 2) * h\n","                gts.append([x1, y1, x2, y2])\n","\n","    matched_gt = set()\n","    for pred in preds:\n","        found = False\n","        for i, gt in enumerate(gts):\n","            iou = compute_iou(pred[:4], gt)\n","            if iou >= 0.5 and i not in matched_gt:\n","                TP += 1\n","                matched_gt.add(i)\n","                found = True\n","                break\n","        if not found:\n","            FP += 1\n","    FN += len(gts) - len(matched_gt)\n","\n","# ‚úÖ Precision Í≥ÑÏÇ∞\n","precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n","\n","# ‚úÖ Ï∂úÎ†•\n","print(f\"\\nüéØ GT Precision ÌèâÍ∞Ä Í≤∞Í≥º\\n{'-'*40}\")\n","print(f\"True Positive (TP) : {TP}\")\n","print(f\"False Positive (FP): {FP}\")\n","print(f\"False Negative (FN): {FN}\")\n","print(f\"Precision           : {precision:.4f}\")\n","print(f\"{'-'*40}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJanlKrCzkEn","executionInfo":{"status":"ok","timestamp":1748854074894,"user_tz":-540,"elapsed":7799,"user":{"displayName":"indi sol","userId":"02918963309027256120"}},"outputId":"d77eb29f-07aa-4b9a-a71a-3fbec3c7d490"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_003900_jpg.rf.06fe6115c628ec86952551b3a9fa853c.jpg: 960x960 1 gown_on, 5 hairnet_ons, 4 persons, 11.8ms\n","Speed: 14.1ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_003540_jpg.rf.9e6ed26ccc834055586104e48af7b389.jpg: 960x960 3 gown_ons, 3 hairnet_ons, 4 persons, 11.8ms\n","Speed: 8.2ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_002100_jpg.rf.e99991537773641016ab9f0ef5f8cb59.jpg: 960x960 6 gown_ons, 9 hairnet_ons, 11 persons, 11.8ms\n","Speed: 8.4ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_001065_jpg.rf.30c387f314d6c1653f761a1f5b66de9c.jpg: 960x960 5 gown_ons, 5 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_000390_jpg.rf.6f7f580c9d71f7b3f8746a024de773bc.jpg: 960x960 2 gown_ons, 2 hairnet_ons, 2 persons, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_000120_jpg.rf.551e16c2ab8cb2ad28b6a400dba9e85c.jpg: 960x960 3 gown_ons, 4 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 2.1ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_000420_jpg.rf.e94f581e0e12617920ba8238faed8802.jpg: 960x960 2 gown_ons, 3 hairnet_ons, 3 persons, 11.8ms\n","Speed: 7.9ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_002610_jpg.rf.79b224edefee4710b638667a6a358e42.jpg: 960x960 3 gown_ons, 5 hairnet_ons, 6 persons, 11.8ms\n","Speed: 7.9ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_002670_jpg.rf.b6e9b3d2a78c2e0881e340bd76c47de0.jpg: 960x960 4 gown_ons, 6 hairnet_ons, 6 persons, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_002280_jpg.rf.57c74d63d4e7225bc8d63e12d4c06662.jpg: 960x960 2 gown_ons, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_000765_jpg.rf.3f943cbeb7133d83f0538f05f1aa8834.jpg: 960x960 5 gown_ons, 5 hairnet_ons, 7 persons, 14.6ms\n","Speed: 8.4ms preprocess, 14.6ms inference, 2.7ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_000570_jpg.rf.c4f4b3967d2ea5c2dd408247712cf149.jpg: 960x960 1 gown_on, 5 hairnet_ons, 6 persons, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_007770_jpg.rf.c27bd9c630b379d81447b24411d6e4ff.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.9ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_007470_jpg.rf.6b6ff7b5d9e6a694a37bdd5c2c503ec6.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005220_jpg.rf.32a8dbdeff5350e2a943aafd98601aad.jpg: 960x960 2 gown_ons, 5 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_007620_jpg.rf.1e6aff5af99516a71bf44ea2f3ef20e8.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 8.0ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005610_jpg.rf.1986d06abe6bdccf134eb17e29b5ac84.jpg: 960x960 2 gown_ons, 5 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_006720_jpg.rf.0a8e78637eccb83be07647842bc73a99.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.6ms preprocess, 11.8ms inference, 2.4ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_013530_jpg.rf.3c35877dd0d9d62712d7f354f15d73eb.jpg: 960x960 2 gown_ons, 3 hairnet_ons, 2 persons, 11.8ms\n","Speed: 7.6ms preprocess, 11.8ms inference, 3.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_012615_jpg.rf.71fd8eaab950ef1fd7ed6d710491b30f.jpg: 960x960 2 gown_ons, 5 hairnet_ons, 7 persons, 11.8ms\n","Speed: 8.4ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011700_jpg.rf.383c90e9479d1d28abb776f9dc5fcc8d.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 8.1ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_013260_jpg.rf.135bdd89949d27046a89dcaccc17e0a4.jpg: 960x960 2 gown_ons, 3 hairnet_ons, 3 persons, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_012600_jpg.rf.a5665dd993afba9577b3f663c01bc9d1.jpg: 960x960 2 gown_ons, 5 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.9ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011265_jpg.rf.467d3fe02fdbb75c99676dad4d219494.jpg: 960x960 2 gown_ons, 5 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011865_jpg.rf.08cd9d9c5fdef7ecf1a9a130ec0039ee.jpg: 960x960 2 gown_ons, 5 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.3ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_010890_jpg.rf.b877f94e87c11d8c2487ce4e449c5645.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011250_jpg.rf.57fd7e33f446a04c1115cfef203924fb.jpg: 960x960 2 gown_ons, 4 hairnet_ons, 4 persons, 11.8ms\n","Speed: 8.1ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_009240_jpg.rf.202b3b64d92beec194cbbaa687f78228.jpg: 960x960 (no detections), 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 0.8ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011490_jpg.rf.e951aedcdf46ec3b2b30d9c0eb9e7cf0.jpg: 960x960 3 gown_ons, 5 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_009270_jpg.rf.e060b717bf94495072c0eabb6707a5cd.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011250_jpg.rf.ebdab796f85d8cd2c591391e40f4345b.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 8.1ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008160_jpg.rf.91ce6def0c3dc1f34bd5107722e98b27.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011280_jpg.rf.268debcf6aeabe2919a698ec61688d75.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 7.6ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_009810_jpg.rf.bed2660a4bd22c16a950a02ee7aecfb6.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_009090_jpg.rf.9cc596dfb0980b65b0f6f0721c9457a6.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 13.4ms\n","Speed: 9.0ms preprocess, 13.4ms inference, 2.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_009780_jpg.rf.919d224ac539ca78e5f003e064e8c6c3.jpg: 960x960 3 gown_ons, 5 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008220_jpg.rf.bab0e5cfde4edc04cd8e1ddc38a904df.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 8.0ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011670_jpg.rf.81bb28f35f21efb369eca6d52a99aeed.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.7ms\n","Speed: 7.7ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_011505_jpg.rf.1dba949586f543c19a976519a1ac1967.jpg: 960x960 3 gown_ons, 5 hairnet_ons, 5 persons, 11.7ms\n","Speed: 7.0ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_010560_jpg.rf.135cfce044e3750ff2bdf1424d54fe2e.jpg: 960x960 3 gown_ons, 5 hairnet_ons, 5 persons, 11.7ms\n","Speed: 7.3ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_010710_jpg.rf.20dd5d7c11ae08364f90f9b2c4ccfd53.jpg: 960x960 1 gown_on, 1 person, 11.7ms\n","Speed: 7.8ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008790_jpg.rf.e48a0fdcc3171422764aac0ba4bfaed5.jpg: 960x960 1 hairnet_on, 2 persons, 11.7ms\n","Speed: 7.8ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_007920_jpg.rf.e597005d6d0cf8ad5dad218e4b5f6fa6.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 11.1ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_007830_jpg.rf.a703229e758be0c072f649e04d7b9d75.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 9.1ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008040_jpg.rf.726485215c8b0524b18331d52bfa9f11.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 8.2ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008460_jpg.rf.0aa9ab109691e4234ddb275209c74283.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 8.4ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_008130_jpg.rf.99490738b7f082ea172c40b20fc6e03a.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 9.4ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005250_jpg.rf.06086c96b5cfdb901779730f1fdd477c.jpg: 960x960 2 gown_ons, 5 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_004080_jpg.rf.8bc55d6e218e6cfc665cf4b2ab1d8d0a.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_004200_jpg.rf.af89ea3763be8c7622685338a580d5f9.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.5ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005730_jpg.rf.4cff361c34e5d2416f2bf6e7a6e4b36d.jpg: 960x960 1 person, 11.8ms\n","Speed: 7.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005940_jpg.rf.674e3706525111ffe6337003635748c0.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_004020_jpg.rf.cb86c1ef6997892ff51d6f53148da87e.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_005700_jpg.rf.9086396df37d8f8b3894ebde2faf2691.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_004455_jpg.rf.0a24fbe290b51d36228e56808bdbd151.jpg: 960x960 2 gown_ons, 4 hairnet_ons, 4 persons, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_006345_jpg.rf.800cc35401cf9f6ccf57111f7fe117b1.jpg: 960x960 1 gown_on, 4 hairnet_ons, 3 persons, 11.8ms\n","Speed: 7.4ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_004110_jpg.rf.aa0273b0afee1249cdf0b0c8ec19d459.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.7ms\n","Speed: 7.4ms preprocess, 11.7ms inference, 1.8ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_003870_jpg.rf.c43ad041f9919eb2b0f5486c2827001e.jpg: 960x960 1 gown_on, 1 hairnet_on, 2 persons, 11.7ms\n","Speed: 7.1ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_002130_jpg.rf.231d276a39bb109cefd61e91bd952d43.jpg: 960x960 6 gown_ons, 9 hairnet_ons, 9 persons, 11.8ms\n","Speed: 7.2ms preprocess, 11.8ms inference, 2.3ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_001770_jpg.rf.63b5efd09e2dd5a59b9d89d0e0b09a34.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 8.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_006660_jpg.rf.f3656c1181f25c2ba0903627be2db732.jpg: 960x960 2 gown_ons, 4 hairnet_ons, 4 persons, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_028410_jpg.rf.899f1e42354ece84bcbd72c0e25d0bd1.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_028200_jpg.rf.3938a39f46bef9a9449363fc70fc1773.jpg: 960x960 2 gown_ons, 1 hairnet_on, 2 persons, 11.8ms\n","Speed: 7.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_027570_jpg.rf.92cbb5ee86f1e0b2fb5af813515ae660.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_027645_jpg.rf.06928ef4e6b3a849eecbe25cc911704b.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_026685_jpg.rf.ec067fd51b4b6152d833667092743ed3.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 7.9ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_026655_jpg.rf.447f90f0a436bd1b5d89a28260721259.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_024525_jpg.rf.58948ce8dafc91bbf3f8a121476ec15c.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 8.4ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_026640_jpg.rf.43020d85eacc52184ca830ad91eff070.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022350_jpg.rf.7be7215a10c97f566e2290b82fa29dae.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 8.0ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_021600_jpg.rf.5bcda8a28f60e935e235513c588f1999.jpg: 960x960 3 gown_ons, 2 hairnet_ons, 3 persons, 11.8ms\n","Speed: 8.1ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_021750_jpg.rf.8724b07e796175ffd88179f05a1a9e3b.jpg: 960x960 2 gown_ons, 2 hairnet_ons, 2 persons, 11.8ms\n","Speed: 7.6ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022380_jpg.rf.7ad0069737c77e0bee1687db2c85341f.jpg: 960x960 4 gown_ons, 4 hairnet_ons, 5 persons, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_021555_jpg.rf.bc321a759dc102c1e1a62d453f43f063.jpg: 960x960 2 gown_ons, 2 hairnet_ons, 2 persons, 11.8ms\n","Speed: 7.6ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022620_jpg.rf.2670dcfc2f768d5cc819ef29d0cb3b5a.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_021480_jpg.rf.345b0c6c0d4019a885111cdbb84eb6db.jpg: 960x960 2 gown_ons, 2 hairnet_ons, 2 persons, 11.8ms\n","Speed: 7.9ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022515_jpg.rf.95f94f9d8ad972e8c62704b1a737069c.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022500_jpg.rf.556759515a8b8247e9fceb4faa335cde.jpg: 960x960 3 gown_ons, 4 hairnet_ons, 4 persons, 11.8ms\n","Speed: 8.2ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_027525_jpg.rf.effd79cad391f0c65d383201ffb48e51.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 7.6ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_024255_jpg.rf.d1f1069a35f5dedd5a87c8470a01bd2a.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 7.6ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_026715_jpg.rf.5e7a5effd2af1f767a438d0e4dcb8b13.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_022665_jpg.rf.58a5d1797c71d7fa4f731673441b24aa.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 7.8ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_023370_jpg.rf.506e4ac41b5e5ca18666cab55f79dd45.jpg: 960x960 1 gown_on, 2 hairnet_ons, 2 persons, 11.8ms\n","Speed: 7.7ms preprocess, 11.8ms inference, 2.4ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_025215_jpg.rf.752bb28aa5fac15be09b9742f3a22968.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 9.2ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_023940_jpg.rf.b5119e06fed50378aa027f74edc84d6d.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 8.0ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_024510_jpg.rf.4051092856f9fbfd68c309b738fb194a.jpg: 960x960 1 gown_on, 1 person, 12.8ms\n","Speed: 10.8ms preprocess, 12.8ms inference, 2.4ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_023100_jpg.rf.fc7faf98b1bbe36073850a69b48b3f0f.jpg: 960x960 2 gown_ons, 3 hairnet_ons, 3 persons, 13.4ms\n","Speed: 11.3ms preprocess, 13.4ms inference, 2.4ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020730_jpg.rf.8cbfe8aa0f5dd7860662244584d15c79.jpg: 960x960 1 gown_on, 3 hairnet_ons, 2 persons, 14.2ms\n","Speed: 12.4ms preprocess, 14.2ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_021510_jpg.rf.874e88b869e35bd60c8e562c50d226fc.jpg: 960x960 4 gown_ons, 4 hairnet_ons, 4 persons, 11.9ms\n","Speed: 13.9ms preprocess, 11.9ms inference, 2.3ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020475_jpg.rf.08b49c17e991643da30ac760b071c9a0.jpg: 960x960 1 hairnet_on, 1 person, 15.9ms\n","Speed: 12.5ms preprocess, 15.9ms inference, 2.4ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020250_jpg.rf.3235409a730b0190afda22efb4ca6ea9.jpg: 960x960 2 gown_ons, 1 hairnet_on, 2 persons, 11.9ms\n","Speed: 13.2ms preprocess, 11.9ms inference, 2.3ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020775_jpg.rf.604861e9010100849057c323e3e9fca2.jpg: 960x960 1 gown_on, 1 hairnet_on, 2 persons, 11.9ms\n","Speed: 13.3ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020640_jpg.rf.99f6dec3ef6c5671422d1f42ec975c8b.jpg: 960x960 1 gown_on, 1 hairnet_on, 2 persons, 11.8ms\n","Speed: 11.7ms preprocess, 11.8ms inference, 2.1ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_020235_jpg.rf.3b7a6d851c67f723065ad8843215a0f5.jpg: 960x960 4 gown_ons, 4 hairnet_ons, 5 persons, 11.8ms\n","Speed: 11.6ms preprocess, 11.8ms inference, 2.1ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_016260_jpg.rf.4c819edc1cf671ffc36defdc67a3269c.jpg: 960x960 5 gown_ons, 6 hairnet_ons, 6 persons, 11.9ms\n","Speed: 11.8ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_018630_jpg.rf.f9d317ef5a793e2071fbcc096ef1bff7.jpg: 960x960 1 gown_on, 1 person, 11.9ms\n","Speed: 11.8ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_018690_jpg.rf.722708915f463775e556315904125843.jpg: 960x960 1 gown_on, 1 person, 11.9ms\n","Speed: 12.8ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_019290_jpg.rf.0e388c392a34d1eec4de4873e2f12de5.jpg: 960x960 4 gown_ons, 3 hairnet_ons, 3 persons, 15.5ms\n","Speed: 12.9ms preprocess, 15.5ms inference, 2.1ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_018330_jpg.rf.1d3c679c7a49e457df9019e3297e6750.jpg: 960x960 1 gown_on, 1 person, 11.9ms\n","Speed: 11.7ms preprocess, 11.9ms inference, 2.2ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_019500_jpg.rf.7d538b346c9f1d485f260596bd9e40eb.jpg: 960x960 1 gown_on, 1 person, 11.9ms\n","Speed: 11.8ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_015270_jpg.rf.8f666285a948a59ac84fd1f68c51dead.jpg: 960x960 2 gown_ons, 3 hairnet_ons, 3 persons, 11.8ms\n","Speed: 11.7ms preprocess, 11.8ms inference, 2.2ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_016140_jpg.rf.f13e12c5e40477c846ea3d18dc527b40.jpg: 960x960 3 gown_ons, 3 hairnet_ons, 4 persons, 12.3ms\n","Speed: 12.6ms preprocess, 12.3ms inference, 2.1ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_019485_jpg.rf.c43bbf492e5f84d1c07ca4b50ed6b201.jpg: 960x960 4 gown_ons, 3 hairnet_ons, 4 persons, 11.9ms\n","Speed: 15.2ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_016620_jpg.rf.4b7a44fab5c9ad4ec224351a81b2e6da.jpg: 960x960 5 gown_ons, 7 hairnet_ons, 7 persons, 11.8ms\n","Speed: 11.4ms preprocess, 11.8ms inference, 2.1ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_015600_jpg.rf.009131807dcdc9d075bd66cc15ef86ae.jpg: 960x960 1 hairnet_on, 1 person, 11.9ms\n","Speed: 11.5ms preprocess, 11.9ms inference, 2.2ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_019080_jpg.rf.e67e6270b2209dadecd52fac8142aebc.jpg: 960x960 1 gown_on, 1 person, 14.1ms\n","Speed: 12.4ms preprocess, 14.1ms inference, 2.8ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_014160_jpg.rf.5937eb7d6c7a87854fad7da42edb20f8.jpg: 960x960 3 gown_ons, 3 hairnet_ons, 4 persons, 26.0ms\n","Speed: 11.9ms preprocess, 26.0ms inference, 2.1ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_014070_jpg.rf.7c7d1ce5b498440f3e8660ff14c4b39f.jpg: 960x960 5 gown_ons, 5 hairnet_ons, 5 persons, 17.9ms\n","Speed: 29.4ms preprocess, 17.9ms inference, 2.4ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_015420_jpg.rf.998440654de69efff75525a09d594b57.jpg: 960x960 4 gown_ons, 4 hairnet_ons, 5 persons, 11.9ms\n","Speed: 11.1ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_015210_jpg.rf.5a6d06712146d4f36517ea9d68378d7f.jpg: 960x960 6 gown_ons, 5 hairnet_ons, 7 persons, 11.9ms\n","Speed: 11.3ms preprocess, 11.9ms inference, 3.1ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_015210_jpg.rf.dfb5f424704477838baaa7834da6eb57.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 11.5ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_014895_jpg.rf.00e73bbc77ab63a79e58eedb71bb9ad2.jpg: 960x960 3 gown_ons, 5 hairnet_ons, 5 persons, 11.9ms\n","Speed: 11.3ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_013560_jpg.rf.4ee2ec5a4467101cf887dc395960af2b.jpg: 960x960 3 gown_ons, 2 hairnet_ons, 3 persons, 11.8ms\n","Speed: 12.9ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_019620_jpg.rf.ecba99ee042d97fae2f61f1960bae888.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 11.3ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_375_jpg.rf.eedf693ebd9cc19bf500d449b4e5f797.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 11.4ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_361_jpg.rf.ebd0297fba31589a389f5916c5422f0a.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 11.8ms\n","Speed: 11.4ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_413_jpg.rf.a07122ecea8ca4cb73d001b87fdcbc78.jpg: 960x960 2 gown_ons, 1 hairnet_on, 2 persons, 11.8ms\n","Speed: 11.8ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_229_jpg.rf.a1bae4976bbef5cc7c7e60c6d40d8d54.jpg: 960x960 6 gown_ons, 5 hairnet_ons, 6 persons, 11.8ms\n","Speed: 11.3ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_59_jpg.rf.29113bafe5b768556da0c0a6d690cb13.jpg: 960x960 4 gown_ons, 4 hairnet_ons, 6 persons, 11.8ms\n","Speed: 13.2ms preprocess, 11.8ms inference, 2.1ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_051060_jpg.rf.b58cdc7956b970368bb9d8545b62878b.jpg: 960x960 3 gown_ons, 3 hairnet_ons, 3 persons, 11.8ms\n","Speed: 14.9ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_031260_jpg.rf.b83b4fb64454ed1b071957c14ec2e759.jpg: 960x960 4 gown_ons, 4 hairnet_ons, 4 persons, 12.8ms\n","Speed: 14.1ms preprocess, 12.8ms inference, 4.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_028425_jpg.rf.b101442f6530449a8ea175b89a3ef8e5.jpg: 960x960 1 gown_on, 1 hairnet_on, 1 person, 13.1ms\n","Speed: 12.9ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_029670_jpg.rf.0dd4f6b007cdfdefbf4bf9681a4754f1.jpg: 960x960 2 gown_ons, 1 hairnet_on, 2 persons, 11.8ms\n","Speed: 11.4ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_065610_jpg.rf.d107446bce4d9f01c2ce332cbd1e46b4.jpg: 960x960 4 gown_ons, 4 hairnet_ons, 4 persons, 11.8ms\n","Speed: 11.2ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_71_jpg.rf.3033eab39133d2fe24bd8f1a763e4a92.jpg: 960x960 5 gown_ons, 5 hairnet_ons, 6 persons, 11.9ms\n","Speed: 11.1ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_030705_jpg.rf.268dd4ac80d2f267f6eb53568984f295.jpg: 960x960 1 hairnet_on, 1 person, 11.8ms\n","Speed: 11.8ms preprocess, 11.8ms inference, 2.1ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_029505_jpg.rf.3466a1d91a082f68d4e2d1fb8e7b3dd1.jpg: 960x960 1 gown_on, 1 person, 11.8ms\n","Speed: 11.0ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_072495_jpg.rf.f6ec06197ccdd337e8d8044f6249977e.jpg: 960x960 5 gown_ons, 5 hairnet_ons, 6 persons, 11.8ms\n","Speed: 11.1ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_037950_jpg.rf.3bfa91e24bdf91a3d899144e5a8134c8.jpg: 960x960 5 gown_ons, 4 hairnet_ons, 5 persons, 11.8ms\n","Speed: 11.0ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_051030_jpg.rf.9570942c61d4aaea5aea9decea39eaa2.jpg: 960x960 3 gown_ons, 3 hairnet_ons, 3 persons, 11.8ms\n","Speed: 11.3ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_028365_jpg.rf.9cd8dc4f80e9ab2af1cb344a0e34aefa.jpg: 960x960 1 gown_on, 2 persons, 11.8ms\n","Speed: 11.4ms preprocess, 11.8ms inference, 2.3ms postprocess per image at shape (1, 3, 960, 960)\n","\n","image 1/1 /content/drive/MyDrive/capstone/capstone_dataset/capstone_project.v9i.yolov5pytorch/valid/images/frame_078915_jpg.rf.1bebcf10df34a4fcc9cb6de93ff6f5a2.jpg: 960x960 5 gown_ons, 5 hairnet_ons, 5 persons, 15.4ms\n","Speed: 11.1ms preprocess, 15.4ms inference, 2.3ms postprocess per image at shape (1, 3, 960, 960)\n","\n","üéØ GT Precision ÌèâÍ∞Ä Í≤∞Í≥º\n","----------------------------------------\n","True Positive (TP) : 864\n","False Positive (FP): 69\n","False Negative (FN): 29\n","Precision           : 0.9260\n","----------------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WdSCNjhaz4yc"},"execution_count":null,"outputs":[]}]}